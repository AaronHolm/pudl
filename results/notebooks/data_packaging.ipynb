{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import datapackage\n",
    "import goodtables\n",
    "import tableschema\n",
    "import json\n",
    "import hashlib\n",
    "import urllib\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "from pudl import init, datastore, settings\n",
    "from pudl.helpers import fix_int_na\n",
    "import pudl.constants as pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remote file path:\n",
    "#msha_data_path = pc.base_data_urls[\"mshamines\"]\n",
    "# Local file path:\n",
    "msha_data_path = os.path.join(settings.DATA_DIR, \"msha\")\n",
    "msha_data_pkg_dir = os.path.join(settings.PUDL_DIR,\"results\",\"data_pkgs\",\"msha\")\n",
    "input_dir = os.path.join(msha_data_pkg_dir, \"input\")\n",
    "\n",
    "# Create a data package to contain our resources.\n",
    "msha_pkg = datapackage.Package(os.path.join(input_dir,\"datapackage.json\"))\n",
    "# Generate an output directory based on the name of the data package\n",
    "output_dir = os.path.join(msha_data_pkg_dir, msha_pkg.descriptor['name'])\n",
    "os.makedirs(os.path.join(output_dir,\"data\"), exist_ok=True)\n",
    "\n",
    "# Dictionary of MSHA data files under msha_data_dir\n",
    "msha_resources = {\n",
    "    \"mines\": {\n",
    "        \"data\": \"Mines.zip\",\n",
    "        \"defs\": \"Mines_Definition_File.txt\"\n",
    "    },\n",
    "    \"controller-operator-history\": {\n",
    "        \"data\": \"ControllerOperatorHistory.zip\",\n",
    "        \"defs\": \"Controller_Operator_History_Definition_File.txt\"\n",
    "    },\n",
    "    \"employment-production-quarterly\": {\n",
    "        \"data\": \"MinesProdQuarterly.zip\",\n",
    "        \"defs\": \"MineSProdQuarterly_Definition_File.txt\"\n",
    "    }\n",
    "#   \"contractor-employment-production-quarterly\": {\n",
    "#       \"data\": \"ContractorProdQuarterly.zip\",\n",
    "#       \"defs\": \"ContractorProdQuarterly_Definition_File.txt\"\n",
    "#   }\n",
    "}\n",
    "\n",
    "for res in msha_resources:\n",
    "    # Create dataframes from input data & definition files (local or remote):\n",
    "    for d in ['data','defs']:\n",
    "        msha_resources[res][f\"{d}_df\"] = pd.read_csv(f\"{msha_data_path}/{msha_resources[res][d]}\",\n",
    "                                               delimiter=\"|\",\n",
    "                                               encoding=\"iso-8859-1\")\n",
    "    # Read the input tabular data resource JSON file we've prepared\n",
    "    msha_resources[res][\"json\"] = json.load(open(os.path.join(input_dir,f\"{res}.json\")))\n",
    "    \n",
    "# OMFG even the MSHA data is broken. *sigh*\n",
    "msha_resources[\"employment-production-quarterly\"][\"data_df\"].columns = \\\n",
    "    list(msha_resources[\"employment-production-quarterly\"][\"defs_df\"]['COLUMN_NAME'])\n",
    "\n",
    "for res in msha_resources:\n",
    "    # Convert the definitions to a dictionary of field descriptions\n",
    "    field_desc = msha_resources[res][\"defs_df\"].set_index('COLUMN_NAME').to_dict()['FIELD_DESCRIPTION']\n",
    "\n",
    "    # Set the description attribute of the fields in the schema using field descriptions.\n",
    "    for field in msha_resources[res][\"json\"][\"schema\"][\"fields\"]:\n",
    "        field['description'] = field_desc[field['name']]\n",
    "    msha_resources[res][\"resource\"] = datapackage.Resource(descriptor=msha_resources[res][\"json\"])\n",
    "    \n",
    "    # Make sure we didn't miss or re-name any fields accidentally\n",
    "    json_fields = msha_resources[res][\"resource\"].schema.field_names\n",
    "    defs_fields = list(msha_resources[res][\"defs_df\"]['COLUMN_NAME'])\n",
    "    data_fields = list(msha_resources[res]['data_df'].columns)\n",
    "    assert json_fields == defs_fields, \"json vs. defs missing field: {}\".format(set(json_fields).symmetric_difference(set(defs_fields)))\n",
    "    assert data_fields == defs_fields, \"data vs. defs missing field: {}\".format(set(data_fields).symmetric_difference(set(defs_fields)))\n",
    "    msha_resources[res][\"resource\"].infer()\n",
    "    msha_resources[res][\"resource\"].commit()\n",
    "    \n",
    "    # Need to clean up the integer NA values in the data before outputting:\n",
    "    for field in msha_resources[res][\"resource\"].schema.field_names:\n",
    "        if msha_resources[res][\"resource\"].schema.get_field(field).type == 'integer':\n",
    "            msha_resources[res][\"data_df\"][field] = fix_int_na(msha_resources[res][\"data_df\"][field])\n",
    "\n",
    "    # Force boolean values to use canonical True/False values.\n",
    "    for field in msha_resources[res][\"resource\"].schema.field_names:\n",
    "        if msha_resources[res][\"resource\"].schema.get_field(field).type == 'boolean':\n",
    "            msha_resources[res][\"data_df\"][field] = msha_resources[res][\"data_df\"][field].replace('Y',True)\n",
    "            msha_resources[res][\"data_df\"][field] = msha_resources[res][\"data_df\"][field].replace('N',False)\n",
    "\n",
    "    # the data itself goes in output -- this is what we're packaging up\n",
    "    output_csv = os.path.join(output_dir,\"data\",f\"{res}.csv\")\n",
    "    msha_resources[res]['data_df'].to_csv(output_csv, index=False)\n",
    "    \n",
    "    # calculate some useful information about the output file, and add it to the resource:\n",
    "    # resource file size:\n",
    "    msha_resources[res][\"resource\"].descriptor[\"bytes\"] = os.path.getsize(output_csv)\n",
    "    \n",
    "    # resource file hash:\n",
    "    BLOCKSIZE = 65536\n",
    "    hasher = hashlib.sha1()\n",
    "    with open(output_csv, 'rb') as afile:\n",
    "        buf = afile.read(BLOCKSIZE)\n",
    "        while len(buf) > 0:\n",
    "            hasher.update(buf)\n",
    "            buf = afile.read(BLOCKSIZE)\n",
    "\n",
    "    msha_resources[res][\"resource\"].descriptor[\"hash\"] = f\"sha1:{hasher.hexdigest()}\"\n",
    "\n",
    "    # Check our work...\n",
    "    if not msha_resources[res][\"resource\"].valid:\n",
    "        print(f\"TABULAR DATA RESOURCE {res} IS NOT VALID.\")\n",
    "    \n",
    "    # Add the completed resource to the data package\n",
    "    msha_pkg.add_resource(descriptor=msha_resources[res][\"resource\"].descriptor)\n",
    "\n",
    "# Automatically fill in some additional metadata\n",
    "msha_pkg.infer();\n",
    "\n",
    "# Timestamp indicating when packaging occured\n",
    "msha_pkg.descriptor['created'] = datetime.datetime.utcnow().replace(microsecond=0).isoformat()+'Z'\n",
    "# Have to set this to 'data-package' rather than 'tabular-data-package' due to a DataHub.io bug\n",
    "msha_pkg.descriptor['profile'] = 'data-package'\n",
    "msha_pkg.commit()\n",
    "\n",
    "# save the datapackage\n",
    "if not msha_pkg.valid:\n",
    "    print(\"MSHA DATA PACKAGE IS NOT VALID.\")\n",
    "msha_pkg.save(os.path.join(output_dir,'datapackage.json'));\n",
    "\n",
    "# Validate some of the data...\n",
    "report = goodtables.validate(os.path.join(output_dir,'datapackage.json'), row_limit=10000)\n",
    "if not report['valid']:\n",
    "    print(\"MSHA DATA TABLES FAILED TO VALIDATE\")\n",
    "    pprint(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://arlweb.msha.gov/OpenGovernmentData/DataSets/NewFile.zip'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_parts = urllib.parse.urlparse(pc.base_data_urls['mshamines'])\n",
    "new_path = url_parts.path + '/' + \"NewFile.zip\"\n",
    "urllib.parse.urlunparse(list(url_parts[0:2])+[new_path,'','',''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
